{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Summarizer Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this .ipynb file locally your system must have python 3.11 or higher version installed in it. This is the pre Requisite of this Project.\n",
    "\n",
    "Recommended to Follow this YouTube Video : [Explanation Video](https://youtu.be/dnNRV3mdzUo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "- install all the necesary libraries needed.\n",
    "\n",
    "<br>\n",
    "\n",
    "for creating the virtual environment to install the required libraries then you can run the following code in your command prompt.\n",
    "\n",
    "`python -m venv .venv`\n",
    "\n",
    "`source .venv/Scripts/activate`(for Linux or MacOS) / `.venv\\Scripts\\activate` (for Windows)\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "or if you want to install these libraries globally then directly run the following cell. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Required Import**\n",
    "- **os** - for interacting with the OS like FileSystem Management.\n",
    "- **logging** - for emitting log messages while program is running. (optional to import)\n",
    "- **time** - for handling time-related operations.\n",
    "- **random** - for generating random numbers and manipulate collections like lists, strings, or booleans.\n",
    "- **typing** - helps developers write statically typed code in a dynamically typed language.\n",
    "- **dotenv** - for managing environment variables for a Python application.\n",
    "- **pymupdf4llm** - for extracting text from pdf to markdown file and to work with LLM.\n",
    "- **textgrad** - to optimize text by implementing LLM-gradients pipelines.\n",
    "- **streamlit** - allows users to create and share data science and machine learning web apps.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Gradient is an LLM development platform** that offers simple web APIs for fine-tuning, embeddings, and inference on state-of-the-art open-source models\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import pymupdf4llm\n",
    "import textgrad as tg\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "-  set up the logging configuration for the application. to monitor activity while program is running. Ex :-\n",
    "\n",
    "![logging_terminal_image.png](images/logging_terminal_image.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- Create your oen **Grop API Key** and paste that here in the place of \"GROQ_API_KEY\" variable.\n",
    "\n",
    "- Steps to Get Groq API Key:\n",
    "1. Go to `https://groq.com/` ang gor to DEV CONSOLE.\n",
    "![Groq_interface](images/Groq.png)\n",
    "2. Then go to API Keys and then click on Create API Key.\n",
    "![Groq_dashboard](images/Groq_dashboard.png)\n",
    "3. Then give a name anf click on submit.\n",
    "![api_key_generation](images/Api_key_generation.png)\n",
    "4. Then open the terminal and create an environment file `.env` and write the API key as follow.\n",
    "\n",
    "Example :- `GROQ_API_KEY = \"sfergdfvxdaw_XYZ_API_KEY\"`\n",
    "\n",
    "![environment_setup](images/Environment_setup.png)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the GROQ_API_KEY environment variable\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "assert GROQ_API_KEY, \"Please set the GROQ_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the summarization pipeline\n",
    "class PipelineConfig:\n",
    "\n",
    "    model_name: list = [\n",
    "        \"gemma-7b-it\",\n",
    "        \"gemma2-9b-it\",\n",
    "        \"llama-3.1-70b-versatile\",\n",
    "        \"llama-3.1-8b-instant\",\n",
    "        \"llama-3.2-11b-text-preview\",\n",
    "        \"llama3-8b-8192\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "    ]\n",
    "    # here temperature is a parameter in a LLM that controls the randomness of the model's output.\n",
    "    temperature: float = 0.7\n",
    "    # to authenticate the user when request is made.\n",
    "    api_key: str = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles PDF reading and text extraction\n",
    "class PDFProcessor: \n",
    "\n",
    "    @staticmethod\n",
    "    def read_pdf(file_path: str) -> List[str]:\n",
    "        try:\n",
    "            # Extract text from PDF file using pymupdf4llm.\n",
    "            llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "            doc = llama_reader.load_data(file_path)\n",
    "            return [page.text for page in doc]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading PDF: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-end pipeline for PDF summarization\n",
    "class SummarizationPipeline:\n",
    "    \n",
    "    # Initialize the pipeline with the configuration\n",
    "    def __init__(self):\n",
    "        self.config = PipelineConfig()\n",
    "        self.model_no = 0\n",
    "\n",
    "    # Initialize the LLM model  \n",
    "    def initialize_model(self):\n",
    "        llm = tg.get_engine(f\"groq-{self.config.model_name[self.model_no]}\")\n",
    "        tg.set_backward_engine(llm, override=True)\n",
    "        return tg.BlackboxLLM(llm)\n",
    "\n",
    "    # Retry the function with backoff in case of rate limit hit\n",
    "    def retry_with_backoff(self, func, *args, **kwargs):\n",
    "        backoff_time = 5\n",
    "        max_backoff_time = 60\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception:\n",
    "                logger.warning(f\"Rate limit hit, retrying in {backoff_time} seconds...\")\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time = min(\n",
    "                    max_backoff_time, backoff_time * 2 + random.uniform(0, 1)\n",
    "                )\n",
    "\n",
    "    # Process a batch of text\n",
    "    def process_batch(self, batch_text: str) -> tg.Variable:\n",
    "        system_prompt = tg.Variable(\n",
    "            value=f\"Here's a financial document. Provide a concise summary highlighting key takeaways. \\nText: {batch_text}\",\n",
    "            requires_grad=True,\n",
    "            role_description=\"system_prompt\",\n",
    "        )\n",
    "        evaluation_instr = (\n",
    "            \"If nothing is important (like header, footer, introduction, title page, etc.) \"\n",
    "            \"then just output 'No important information found'. Else, highlight the important \"\n",
    "            \"information in key points. Do not add any additional information \"\n",
    "        )\n",
    "        answer = self.retry_with_backoff(self.initialize_model(), system_prompt)\n",
    "        self.optimize_answer(answer, evaluation_instr)\n",
    "        return answer\n",
    "\n",
    "    # Optimize the answer using the loss function\n",
    "    def optimize_answer(self, answer: tg.Variable, evaluation_instr: str):\n",
    "        optimizer = tg.TGD(parameters=[answer])\n",
    "        loss_fn = tg.TextLoss(evaluation_instr)\n",
    "        loss = loss_fn(answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Process the PDF file\n",
    "    def process(self, file_path: str) -> str:\n",
    "        try:\n",
    "            with st.spinner(\"Reading and processing PDF...\"):\n",
    "                pages = PDFProcessor.read_pdf(file_path)\n",
    "                # Combine pages into batches\n",
    "                batch_size = 5\n",
    "                batches = [\n",
    "                    \" \".join(pages[i : i + batch_size])\n",
    "                    for i in range(0, len(pages), batch_size)\n",
    "                ]\n",
    "\n",
    "                progress_bar = st.progress(0)\n",
    "                batch_summaries = []\n",
    "                for i, batch in enumerate(batches):\n",
    "                    batch_summaries.append(self.process_batch(batch))\n",
    "                    progress_bar.progress((i + 1) / len(batches))\n",
    "                    self.model_no += 1\n",
    "                    self.model_no %= len(self.config.model_name) - 1\n",
    "\n",
    "                combined_text = \" \".join([batch.value for batch in batch_summaries])\n",
    "                final_summary = self.summarize_document(combined_text)\n",
    "                return final_summary.value\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error processing PDF: {e}\")\n",
    "            logger.error(f\"Error processing PDF: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Summarize the document\n",
    "    def summarize_document(self, text: str) -> tg.Variable:\n",
    "        system_prompt = tg.Variable(\n",
    "            value=f\"Here's a financial document. Provide a concise summary highlighting key takeaways.\\nText: {text}\",\n",
    "            requires_grad=True,\n",
    "            role_description=\"system_prompt\",\n",
    "        )\n",
    "        evaluation_instr = (\n",
    "            \"Provide a concise summary of the document. Be very careful to not exclude the most \"\n",
    "            \"important information and provide correct statistical data. Keep the summary in specific \"\n",
    "            \"points and do not add any additional information not given in the text.\"\n",
    "        )\n",
    "        final_answer = self.retry_with_backoff(self.initialize_model(), system_prompt)\n",
    "        self.optimize_answer(final_answer, evaluation_instr)\n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Streamlit UI\n",
    "    st.title(\"PDF Summarization Tool\")\n",
    "    st.write(\"Upload a PDF file to generate a summary of its contents.\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        # Save the uploaded file temporarily\n",
    "        with st.spinner(\"Saving uploaded file...\"):\n",
    "            temp_path = f\"temp_{uploaded_file.name}\"\n",
    "            with open(temp_path, \"wb\") as f:\n",
    "                f.write(uploaded_file.getvalue())\n",
    "\n",
    "        try:\n",
    "            # Process the uploaded PDF\n",
    "            pipeline = SummarizationPipeline()\n",
    "            summary = pipeline.process(temp_path)\n",
    "\n",
    "            st.subheader(\"Summary\")\n",
    "            st.write(summary)\n",
    "\n",
    "        finally:\n",
    "            # Cleanup temporary file\n",
    "            if os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 style=\"color:red\"><b> YOU WILL GET ERROR !!! </b></h1>\n",
    "<p>in Jupyter Notebook, this warning might appear if certain modules or tools expect a ScriptRunContext, which is typically relevant for environments like Azure ML or other managed cloud platforms.</p>\n",
    "<p>(in our case this problem occured due to <b>StreamLit</b>)</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "to implement this project we have to run **pipeline.py** file.\n",
    "\n",
    "so in terminal run `streamlit run pipeline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Website UI\n",
    "\n",
    "![UI_1](images/Streamlit_UI_1.png)\n",
    "\n",
    "![UI_2](images/Streamlit_UI_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
